{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "df = pd.read_csv('FinSalesPriceData_train.csv')\n",
    "\n",
    "\n",
    "df_test = pd.read_csv('FinSalesPriceData_test.csv')\n",
    "rival = pd.read_csv('CompetitorPriceData.csv')\n",
    "\n",
    "df['calendar_day'] = pd.to_datetime(df['calendar_day'])\n",
    "df_test['calendar_day'] = pd.to_datetime(df_test['calendar_day'])\n",
    "rival['fiscal_week_start_date'] = pd.to_datetime(rival['fiscal_week_start_date'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df = pd.read_csv('/Users/clj/Desktop/capstone/BigW_df/FinSalesPriceData_train.csv')\n",
    "\n",
    "\n",
    "# df_test = pd.read_csv('/Users/clj/Desktop/capstone/BigW_df/FinSalesPriceData_test.csv')\n",
    "# rival = pd.read_csv('/Users/clj/Desktop/capstone/BigW_df/CompetitorPriceData.csv')\n",
    "\n",
    "# df['calendar_day'] = pd.to_datetime(df['calendar_day'])\n",
    "# df_test['calendar_day'] = pd.to_datetime(df_test['calendar_day'])\n",
    "# rival['fiscal_week_start_date'] = pd.to_datetime(rival['fiscal_week_start_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gst_flag'] = df['gst_flag'].replace({'Y':1,'N':0})\n",
    "df_test['gst_flag'] = df_test['gst_flag'].replace({'Y':1,'N':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### COGS function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cogs(sales_amount,gross_profit,gst_flag):\n",
    "    sales_amount_ex_gst = sales_amount/(1+0.1*gst_flag)\n",
    "    return (sales_amount_ex_gst - gross_profit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Gross profit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gross_profit(unit_sold, promo_sales, regular_sales, cogs,gst_flag):\n",
    "    gst = (0.1 if gst_flag == 1 else 0)\n",
    "    sales_ex_gst = (promo_sales + regular_sales)/(1+ gst)\n",
    "    return (sales_ex_gst - (unit_sold * cogs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Fill NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['promo_price'] = np.where((df['sell_price'] == df['promo_price']) \n",
    "                             | (df['promo_price'].isnull()), 0, df['promo_price'])\n",
    "df_test['promo_price'] = np.where((df_test['sell_price'] == df_test['promo_price']) \n",
    "                             | (df_test['promo_price'].isnull()), 0, df_test['promo_price'])\n",
    "\n",
    "df['promo_sales'] = np.where((df['promo_sales'].isnull()), 0, df['promo_price'])\n",
    "df_test['promo_sales'] = np.where((df_test['promo_sales'].isnull()), 0, df_test['promo_price'])\n",
    "\n",
    "df['scanback'] = np.where((df['scanback'].isnull()), 0, df['promo_price'])\n",
    "df_test['scanback'] = np.where((df_test['scanback'].isnull()), 0, df_test['promo_price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### FF filling dettol price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 筛选出 article_id 为 695147 的行索引\n",
    "indices = df[(df['article_id'] == 695147) & (df['sales_units'] > 0)].index\n",
    "\n",
    "# 对这些行计算 sell_price 的估算值\n",
    "df.loc[indices, 'estimated_sell_price'] = df.loc[indices, 'sales_amount'] / df.loc[indices, 'sales_units']\n",
    "\n",
    "# 对整个 DataFrame 应用 forward fill 来填补 estimated_sell_price\n",
    "df['estimated_sell_price'] = df['estimated_sell_price'].fillna(method='ffill')\n",
    "\n",
    "# 现在，您可以选择直接更新 sell_price 列\n",
    "df.loc[df['article_id'] == 695147, 'sell_price'] = df.loc[df['article_id'] == 695147, 'estimated_sell_price']\n",
    "\n",
    "# 如果不再需要 estimated_sell_price 列，可以选择删除它\n",
    "df.drop(columns=['estimated_sell_price'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### 移除Deranged 产品"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[(df['cnt_site_art_ranged'] == 0) & (df['sales_units'] != 0)].index, inplace=True)\n",
    "df_test.drop(df_test[(df_test['cnt_site_art_ranged'] == 0) & (df_test['sales_units'] != 0)].index, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### 是否打折"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_promo'] = np.where(df['promo_price'] != 0, 1, 0)\n",
    "df_test['is_promo'] = np.where(df_test['promo_price'] != 0, 1, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### 去掉负数 Gross profit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Identify article_ids (if you haven't already)\n",
    "negative_profit = df.groupby(['category', 'subcategory','article_id']).agg(\n",
    "    gross_profit=('gross_profit', 'sum'),  # Assuming the column name is 'gross_profits'\n",
    "    sales_units=('sales_units', 'sum'),\n",
    ").reset_index()\n",
    "\n",
    "negative_profit_test = df_test.groupby(['category', 'subcategory','article_id']).agg(\n",
    "    gross_profit=('gross_profit', 'sum'),  # Assuming the column name is 'gross_profits'\n",
    "    sales_units=('sales_units', 'sum'),\n",
    ").reset_index()\n",
    "\n",
    "\n",
    "negative_profit_zero_sales_ids = negative_profit[\n",
    "    (negative_profit['gross_profit'] <= 0) & (negative_profit['sales_units'] == 0)\n",
    "]['article_id'].unique()\n",
    "\n",
    "negative_profit_zero_sales_ids_test = negative_profit_test[\n",
    "    (negative_profit_test['gross_profit'] <= 0) & (negative_profit_test['sales_units'] == 0)\n",
    "]['article_id'].unique()\n",
    "\n",
    "# Step 2: Find indexes of rows in df to drop\n",
    "indexes_to_drop = df[df['article_id'].isin(negative_profit_zero_sales_ids)].index\n",
    "\n",
    "indexes_to_drop_test = df_test[df_test['article_id'].isin(negative_profit_zero_sales_ids_test)].index\n",
    "\n",
    "# Step 3: Drop these rows in-place from df\n",
    "df.drop(indexes_to_drop, inplace=True)\n",
    "\n",
    "df_test.drop(indexes_to_drop_test, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 合并competitor df\n",
    "merged_df = pd.merge(df, rival, how='left', left_on=['article_id', 'calendar_day'], right_on=['article_id', 'fiscal_week_start_date'])\n",
    "\n",
    "# Renaming the columns from the rival DataFrame to match your query output if necessary\n",
    "merged_df.rename(columns={\n",
    "    'competitor': 'competitor',\n",
    "    'competitor_shelf_price': 'competitor_shelf_price',\n",
    "    'competitor_total_price': 'competitor_total_price'\n",
    "}, inplace=True)\n",
    "\n",
    "# Ensure the DataFrame is sorted by 'article_id' and 'calendar_day' before forward filling\n",
    "merged_df = merged_df.sort_values(by=['article_id', 'calendar_day'])\n",
    "\n",
    "# Forward fill the missing values for the specified columns\n",
    "columns_to_ffill = ['fiscal_week_start_date', 'competitor', 'competitor_shelf_price', 'competitor_total_price']\n",
    "merged_df[columns_to_ffill] = merged_df.groupby('article_id')[columns_to_ffill].ffill()\n",
    "\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 合并competitor df\n",
    "merged_df_test = pd.merge(df_test, rival, how='left', left_on=['article_id', 'calendar_day'], right_on=['article_id', 'fiscal_week_start_date'])\n",
    "\n",
    "# Renaming the columns from the rival DataFrame to match your query output if necessary\n",
    "merged_df_test.rename(columns={\n",
    "    'competitor': 'competitor',\n",
    "    'competitor_shelf_price': 'competitor_shelf_price',\n",
    "    'competitor_total_price': 'competitor_total_price'\n",
    "}, inplace=True)\n",
    "\n",
    "# Ensure the DataFrame is sorted by 'article_id' and 'calendar_day' before forward filling\n",
    "merged_df_test = merged_df_test.sort_values(by=['article_id', 'calendar_day'])\n",
    "\n",
    "# Forward fill the missing values for the specified columns\n",
    "columns_to_ffill = ['fiscal_week_start_date', 'competitor', 'competitor_shelf_price', 'competitor_total_price']\n",
    "merged_df_test[columns_to_ffill] = merged_df_test.groupby('article_id')[columns_to_ffill].ffill()\n",
    "\n",
    "merged_df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr = sales_dummy.corr().round(3)\n",
    "# corr_units = corr.loc['sales_units',:].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_para_index = corr_units.loc[(corr_units > 0.1) | (corr_units < -0.1)]\n",
    "# best_para_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df.to_csv('/Users/clj/Desktop/capstone/BigW_df/bigw_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the fluctuation of gross_profit \n",
    "sns.lineplot(data = train, x='calendar_day', y='gross_profit', color = 'steelblue')\n",
    "plt.ylabel('gross_profit (AUD)')\n",
    "plt.title('The fluctuation of gross_profit')\n",
    "plt.savefig(\"gross_profit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the correlation between the variables and gross_profit\n",
    "train.corr()['gross_profit'].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "corr_matrix = train.corr()\n",
    "\n",
    "# Select correlations of 'sales' with other variables\n",
    "gross_profit_corr = corr_matrix['gross_profit']\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(gross_profit_corr.to_frame(), cmap='crest', annot=True, fmt=\".2f\", cbar=True)\n",
    "plt.title('Correlation of gross profit')\n",
    "plt.savefig('corr of gross profit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sales_amount * sales_unit\n",
    "sns.scatterplot(data=train, x=\"sales_units\", y=\"sales_amount\", hue=\"category\", legend=\"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "sales_units1= trian.groupby('category')['sales_units'].sum()\n",
    "promo_units1= train.groupby('category')['promo_units'].sum()\n",
    "\n",
    "\n",
    "categories = ['Skin & Sun Care', 'Baby Consumables', 'Household Cleaning', 'Personal Hygiene']\n",
    "sales_units = [6383005, 7850024, 11803738, 7811973]\n",
    "promo_units = [3580535, 2643383, 3273582, 3140521]\n",
    "df = pd.DataFrame({'Category': categories, 'Sales Units (%)': sales_units, 'promo_units (%)': promo_units})\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "df.plot(kind='bar', x='Category', ax=ax)\n",
    "ax.set_ylabel('Values')\n",
    "ax.set_title('Sales Data by Category')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scanback Vs sales units\n",
    "plt.scatter(train['scanback'], train['sales_units'], color =\"steelblue\", alpha = 0.8)\n",
    "plt.ylabel('sales_units')\n",
    "plt.xlabel('scanback (AUD)')\n",
    "plt.title('sales_units Vs scanback')\n",
    "plt.savefig('ss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 独家VS非独家\n",
    "merged_df['is_only_supplier'] = (merged_df['competitor'].notna()).astype(int)\n",
    "merged_df_test['is_only_supplier'] = (merged_df_test['competitor'].notna()).astype(int)\n",
    "\n",
    "\n",
    "# number of brands with subcategory (当下细分市场的竞争有多激烈)\n",
    "\n",
    "# Step 1: Group by 'category' and 'subcategory' and count distinct brands\n",
    "brand_count = merged_df.groupby(['category', 'subcategory'])['brand'].nunique().reset_index(name='brand_cnt')\n",
    "brand_count_test = merged_df_test.groupby(['category', 'subcategory'])['brand'].nunique().reset_index(name='brand_cnt')\n",
    "\n",
    "# Step 2: Rank the subcategories within each category based on brand count\n",
    "brand_count['competition_level'] = brand_count.groupby('category')['brand_cnt'].rank(method='min', ascending=False)\n",
    "brand_count_test['competition_level'] = brand_count_test.groupby('category')['brand_cnt'].rank(method='min', ascending=False)\n",
    "\n",
    "# Merge the ranking back to the original DataFrame\n",
    "merged_df = merged_df.merge(brand_count[['category', 'subcategory', 'competition_level']], on=['category', 'subcategory'], how='left')\n",
    "merged_df_test = merged_df_test.merge(brand_count[['category', 'subcategory', 'competition_level']], on=['category', 'subcategory'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "### weekly 处理\n",
    "\n",
    "######## weekly sell_price and promo_price\n",
    "# Creating a 'week' column that represents the week of the year\n",
    "merged_df['week'] = merged_df['calendar_day'].dt.to_period('W')\n",
    "merged_df_test['week'] = merged_df_test['calendar_day'].dt.to_period('W')\n",
    "\n",
    "# # Group by 'article_id' and 'week', and calculate the mean for 'sell_price' and 'promo_price'\n",
    "# weekly_avg_prices = merged_df.groupby(['article_id', 'week']).agg(\n",
    "#     avg_price=('sell_price', 'mean'),\n",
    "#     avg_promo_price=('promo_price', 'mean')\n",
    "# ).reset_index()\n",
    "\n",
    "# weekly_avg_prices_test = merged_df_test.groupby(['article_id', 'week']).agg(\n",
    "#     avg_price=('sell_price', 'mean'),\n",
    "#     avg_promo_price=('promo_price', 'mean')\n",
    "# ).reset_index()\n",
    "\n",
    "# merged_df = pd.merge(merged_df, weekly_avg_prices, on=['article_id', 'week'], how='left')\n",
    "# merged_df_test = pd.merge(merged_df_test, weekly_avg_prices_test, on=['article_id', 'week'], how='left')\n",
    "\n",
    "# Converting to string format\n",
    "# weekly_avg_prices['week'] = weekly_avg_prices['week'].astype(str)\n",
    "\n",
    "\n",
    "\n",
    "######## weekly一起促销产品的数量 within subcategory\n",
    "on_sale = merged_df['is_promo'] == 1\n",
    "on_sale_test = merged_df_test['is_promo'] == 1\n",
    "\n",
    "# Step 2: Count distinct SKUs on sale per week and subcategory\n",
    "competitive_count = merged_df[on_sale].groupby(['week', 'category','subcategory']).agg(\n",
    "    num_skus_on_sale=('article_id', 'nunique')\n",
    ").reset_index()\n",
    "\n",
    "competitive_count_test = merged_df_test[on_sale].groupby(['week', 'category','subcategory']).agg(\n",
    "    num_skus_on_sale=('article_id', 'nunique')\n",
    ").reset_index()\n",
    "\n",
    "# Step 3: Merge this count back to the original DataFrame\n",
    "merged_df = pd.merge(merged_df, competitive_count, on=['week', 'subcategory'], how='left')\n",
    "merged_df_test = pd.merge(merged_df_test, competitive_count_test, on=['week', 'subcategory'], how='left')\n",
    "\n",
    "# Fill NaN values with 0 where there were no on-sale SKUs in the week and subcategory\n",
    "merged_df['num_skus_on_sale'].fillna(0, inplace=True)\n",
    "merged_df_test['num_skus_on_sale'].fillna(0, inplace=True)\n",
    "\n",
    "######## (weekly sell - weekly promo)/weeklysell 打折力度\n",
    "# merged_df['promo_power'] = (merged_df['avg_price'] - merged_df['avg_promo_price']) / merged_df['avg_price']\n",
    "# merged_df_test['promo_power'] = (merged_df_test['avg_price'] - merged_df_test['avg_promo_price']) / merged_df_test['avg_price']\n",
    "\n",
    "# # Handling cases where the weekly sell price might be zero to avoid division by zero errors\n",
    "# merged_df['promo_power'] = merged_df['promo_power'].replace([float('inf'), -float('inf')], 0)\n",
    "# merged_df_test['promo_power'] = merged_df_test['promo_power'].replace([float('inf'), -float('inf')], 0)\n",
    "\n",
    "\n",
    "\n",
    "merged_df['promo_power'] = (merged_df['sell_price'] - merged_df['promo_price']) / merged_df['sell_price']\n",
    "merged_df_test['promo_power'] = (merged_df_test['sell_price'] - merged_df_test['promo_price']) / merged_df_test['sell_price']\n",
    "# Fill NaN values that might have occurred due to both prices being zero or missing promo prices\n",
    "merged_df['promo_power'].fillna(0, inplace=True)\n",
    "merged_df_test['promo_power'].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark the week of day for the given product in given date\n",
    "merged_df['day_of_week'] = merged_df['calendar_day'].dt.day_of_week\n",
    "merged_df_test['day_of_week'] = merged_df_test['calendar_day'].dt.day_of_week\n",
    "\n",
    "# Detect whether the product was sold in weekend\n",
    "merged_df['is_weekend'] = (merged_df['day_of_week'] >= 5).astype(int)\n",
    "merged_df_test['is_weekend'] = (merged_df_test['day_of_week'] >= 5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#promotion active flag\n",
    "merged_df['promotion_active'] = np.where(merged_df['promo_price'] != 0, 1,0)\n",
    "merged_df_test['promotion_active'] = np.where(merged_df_test['promo_price'] != 0, 1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#距离上次打折的时间\n",
    "# Detect promotion end dates\n",
    "merged_df['promo_ends'] = merged_df.groupby('article_id')['promotion_active'].diff().fillna(0).lt(0)\n",
    "merged_df_test['promo_ends'] = merged_df_test.groupby('article_id')['promotion_active'].diff().fillna(0).lt(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate days since last promotion\n",
    "def days_since_last_promo(row,group):\n",
    "    prior_promo_ends = group.loc[(group['calendar_day'] < row['calendar_day']) & group['promo_ends'],'calendar_day']\n",
    "    if not prior_promo_ends.empty:\n",
    "        return (row['calendar_day'] - prior_promo_ends.max()).days\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the function to dataframe row_wise\n",
    "#merged_df['days_since_last_promo'] = merged_df.apply(lambda row: days_since_last_promo(row,merged_df), axis=1)\n",
    "#merged_df_test['days_since_last_promo'] = merged_df_test.apply(lambda row: days_since_last_promo(row,merged_df_test), axis=1)\n",
    "\n",
    "#Mark products that have no promotion occurred before\n",
    "#merged_df['days_since_last_promo'].fillna(-1,inplace=True) # Use -1 to indicate no prior promotion\n",
    "#merged_df_test['days_since_last_promo'].fillna(-1,inplace=True)\n",
    "# Too time consuming, don't run this part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# days_since_last_promo = pd.read_csv('days_since_last_promo.csv')\n",
    "# days_since_last_promo_test = pd.read_csv('days_since_last_promo_test.csv')\n",
    "days_since_last_promo = pd.read_csv('days_since_last_promo.csv')\n",
    "days_since_last_promo_test = pd.read_csv('days_since_last_promo_test.csv')\n",
    "\n",
    "days_since_last_promo = days_since_last_promo.drop_duplicates()\n",
    "days_since_last_promo_test = days_since_last_promo_test.drop_duplicates()\n",
    "\n",
    "days_since_last_promo['calendar_day'] = pd.to_datetime(days_since_last_promo['calendar_day'])\n",
    "days_since_last_promo_test['calendar_day'] = pd.to_datetime(days_since_last_promo_test['calendar_day'])\n",
    "merged_df = pd.merge(merged_df, days_since_last_promo, on=['calendar_day', 'article_id'], how='left')\n",
    "merged_df_test = pd.merge(merged_df_test,days_since_last_promo_test, on=['calendar_day','article_id'])\n",
    "\n",
    "\n",
    "# 聚合 sales_units\n",
    "sales_agg = merged_df.groupby(['article_id', 'calendar_day']).agg({'sales_units': 'sum'}).reset_index()\n",
    "# 选择原始数据集中不想改变的列\n",
    "original_columns = merged_df.drop(columns='sales_units').drop_duplicates()\n",
    "# 左连接聚合后的数据\n",
    "merged_df = pd.merge(original_columns, sales_agg, on=['article_id', 'calendar_day'], how='left').drop_duplicates(subset=['article_id', 'calendar_day'])\n",
    "\n",
    "\n",
    "sales_agg = merged_df_test.groupby(['article_id', 'calendar_day']).agg({'sales_units': 'sum'}).reset_index()\n",
    "original_columns = merged_df_test.drop(columns='sales_units').drop_duplicates()\n",
    "merged_df_test = pd.merge(original_columns, sales_agg, on=['article_id', 'calendar_day'], how='left').drop_duplicates(subset=['article_id', 'calendar_day'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "sell_price = np.array(merged_df_test['sell_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Manually specify which columns are numerical\n",
    "numerical_cols = ['sell_price', 'promo_price', 'cnt_site_art_ranged',\n",
    "       'cnt_site_art_ranged_pstv_soh', 'tot_soh_ranged_sites','num_skus_on_sale','days_since_last_promo']  # Replace with your actual numerical columns\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "# Apply scaling only to the numerical columns\n",
    "merged_df_test[numerical_cols] = scaler.fit_transform(merged_df_test[numerical_cols])\n",
    "merged_df[numerical_cols] = scaler.fit_transform(merged_df[numerical_cols])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#最终花了1个小时跑完，为了节约时间将把这段代码的输出结果保存\n",
    "# stored_col = merged_df[['calendar_day','article_id','days_since_last_promo']].copy()\n",
    "# stored_col.to_csv('days_since_last_promo.csv', index=False)\n",
    "# stored_col_test = merged_df_test[['calendar_day','article_id','days_since_last_promo']].copy()\n",
    "# stored_col_test.to_csv('days_since_last_promo_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = merged_df.copy() \n",
    "sales = sales.drop(['article_id','calendar_day','promo_units','scanback','article_desc','category_x','category_y','brand','segment','brandtype','week','competitor','sales_amount','gross_profit','promo_sales','fiscal_week_start_date','promo_ends'],axis=1)\n",
    "sales_test = merged_df_test.copy()\n",
    "sales_test = sales_test.drop(['article_id','calendar_day','promo_units','scanback','article_desc','category_x','category_y','brand','segment','brandtype','week','competitor','sales_amount','gross_profit','promo_sales','fiscal_week_start_date','promo_ends'],axis=1)\n",
    "sales.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_dummy = pd.get_dummies(sales,drop_first=True)\n",
    "sales_dummy_test = pd.get_dummies(sales_test,drop_first= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gross_profit = merged_df_test.loc[:,'gross_profit']\n",
    "\n",
    "sales_amount = np.array(merged_df_test['sales_amount'])\n",
    "gross_profit = np.array(merged_df_test['gross_profit'])\n",
    "promo_price = np.array(merged_df_test['promo_price'])\n",
    "gst_flag = np.array(merged_df_test['gst_flag'])\n",
    "sales_amount_ex_gst = sales_amount/(1+0.1*gst_flag)\n",
    "cogs = (sales_amount_ex_gst - gross_profit)\n",
    "sales_units = merged_df_test['sales_units']\n",
    "\n",
    "ind_cogs = list(map(lambda cog, units : cog / units if units != 0 else 0, cogs, sales_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "## Model 1 :Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = sales_dummy.loc[:,'sales_units'].copy()\n",
    "train_X = sales_dummy.copy().drop(['sales_units','competitor_shelf_price','competitor_total_price'],axis=1)\n",
    "\n",
    "elastic_net = ElasticNetCV(l1_ratio=[0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99,1], cv=5)\n",
    "elastic = elastic_net.fit(train_X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.shape #(1104605, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic.l1_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 看各变量的重要程度\n",
    "betas = elastic_net.coef_\n",
    "predictors = list(train_X.columns.values[:])\n",
    "indicies = np.argsort(np.abs(betas))[-20:]\n",
    "top_predictors = np.array(predictors)[indicies]\n",
    "top_betas = betas[indicies]\n",
    "\n",
    "plt.figure(figsize = (30, 15))\n",
    "plt.barh(top_predictors, top_betas, alpha=0.5, edgecolor='black')\n",
    "sns.despine()\n",
    "plt.xlabel('Beta coefficient')\n",
    "plt.title('Freature importance for Elastic Net', fontsize = 20)\n",
    "plt.savefig('elastic_var_importance.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = sales_dummy_test.loc[:,'sales_units'].copy()\n",
    "test_X = sales_dummy_test.copy().drop(['sales_units','competitor_shelf_price','competitor_total_price'],axis=1)\n",
    "\n",
    "\n",
    "#test RMSE\n",
    "result = elastic.predict(test_X)\n",
    "print(mean_squared_error(test_y,result, squared= False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RMSE\n",
    "result_training = elastic.predict(train_X)\n",
    "print(mean_squared_error(train_y,result_training, squared= False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_sales_m1 = np.array(result) * sell_price\n",
    "promo_sales_m1 = np.array(result) * 0.1 * np.array(merged_df_test['is_promo']) * promo_price\n",
    "sales_amount_m1 = regular_sales_m1 + promo_sales_m1\n",
    "gross_profit_test_m1 = sales_amount_m1 - (ind_cogs * np.array(result))\n",
    "print(mean_squared_error(gross_profit_test_m1 ,gross_profit, squared= False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_train = train_y - result_training\n",
    "residual_test = test_y - result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = result_training, y = residual_train, label = 'Training set', color = 'blue', s = 60)\n",
    "sns.scatterplot(x = result, y = residual_test, label = 'Test set',color = 'red', s = 60)\n",
    "\n",
    "plt. axhline(0, color = 'black', linestyle = '--',linewidth = 1)\n",
    "plt.xlabel('Sales units (predicted value)')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('residual plot for Elastic net')\n",
    "plt.legend()\n",
    "plt.savefig('residual_plot_elastic_net.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjusted_r2_score(y_true, y_pred, n_features):\n",
    "    n = len(y_true)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - n_features - 1)\n",
    "    return adjusted_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate adj R2 score\n",
    "n_feature = np.count_nonzero(betas)\n",
    "adj_r2 = adjusted_r2_score(test_y,result,n_feature)\n",
    "print(adj_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "## Model 2: Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = RandomForestRegressor(criterion= 'squared_error', min_samples_leaf=10, max_features=100, random_state= 0)\n",
    "\n",
    "parameters = {'n_estimators' : np.arange(100,200,10),\n",
    "              'min_samples_leaf': np.arange(100, 1000),\n",
    "              'max_features': np.arange(1, len(train_X.columns)-1)}\n",
    "\n",
    "\n",
    "random_search = RandomizedSearchCV(RandomForestRegressor(criterion='squared_error', random_state=0), \n",
    "                                   parameters, n_iter=1, cv=5, \n",
    "                                   scoring='neg_mean_squared_error', random_state=0)\n",
    "\n",
    "random_search.fit(train_X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = random_search.best_params_\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(random_state=0,**params)\n",
    "model2 = rf.fit(train_X,train_y)\n",
    "result_rf = model2.predict(test_X)\n",
    "#test RMSE\n",
    "print(mean_squared_error(test_y,result_rf, squared= False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train RMSE\n",
    "result_rf_train = model2.predict(train_X)\n",
    "print(mean_squared_error(train_y,result_rf_train, squared= False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train_X.columns\n",
    "importances = pd.Series(model2.feature_importances_, index=features).sort_values()\n",
    "\n",
    "plt.barh(range(len(importances[-20:])), importances[-20:], tick_label=importances.index[-20:])\n",
    "plt.title('Random Forest Feature Importance')\n",
    "plt.xlabel('Importance'); plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.savefig('importances_rf.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_sales_m2 = np.array(result_rf) * sell_price\n",
    "promo_sales_m2 = np.array(result_rf) * 0.1 * np.array(merged_df_test['is_promo']) * promo_price\n",
    "sales_amount_m2 = regular_sales_m2 + promo_sales_m2\n",
    "gross_profit_test_m2 = sales_amount_m2 - (ind_cogs * np.array(result_rf))\n",
    "print(mean_squared_error(gross_profit_test_m2 ,gross_profit, squared= False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(test_y,result_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_train_rf = train_y - result_rf_train\n",
    "residual_test_rf = test_y - result_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = result_rf_train, y = residual_train_rf, label = 'Training set', color = 'blue', s = 60)\n",
    "sns.scatterplot(x = result_rf, y = residual_test_rf, label = 'Test set',color = 'red', s = 60)\n",
    "\n",
    "plt. axhline(0, color = 'black', linestyle = '--',linewidth = 1)\n",
    "plt.xlabel('Sales units (predicted value)')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('residual plot for Random Forest')\n",
    "plt.legend()\n",
    "plt.savefig('residual_plot_random_forest.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "## Model 3: XGBoosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "**Model training with hyperparameter selection**. We can use GridSearchCV to select the best parameters. The most important hyperparameters to tune is: \n",
    "* `learning_rate`: learning rate\n",
    "* `n_estimators`: number of classifiers to add\n",
    "* `max_depth`: maximum depth of each decision tree model\n",
    "* `subsample`: Controls the fraction of the total training set to use to train each individual tree. By randomly sampling part of the data for each tree, it adds randomness into the model, which can help improve its robustness and prevent overfitting.\n",
    "* `colsample_bytree`: controls the fraction of features used for building each tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load dataset \n",
    "# One-hot encode the categorical variables\n",
    "\n",
    "# X = merged_df[['brandtype','category_subcategory','rank','day_of_week','avg_price', 'avg_promo_price', 'is_promo', 'promo_power', 'num_skus_on_sale']]\n",
    "# X_encoded = pd.get_dummies(X, columns=['brandtype', 'category_subcategory','day_of_week','is_promo',])\n",
    "# y = merged_df['sales_units']\n",
    "\n",
    "# X_test = pd.get_dummies(df_test_merged[['brandtype', 'category_subcategory', 'rank', 'day_of_week', 'avg_price', 'avg_promo_price', 'is_promo', 'num_skus_on_sale']])\n",
    "# X_test = X_test.reindex(columns=X_encoded.columns, fill_value=0)  # Ensure test set has the same features as the training set\n",
    "# y_test = df_test_merged['sales_units']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "xgb_model = XGBRegressor(random_state=0,objective='reg:squarederror',verbosity=2) \n",
    "\n",
    "# Define the parameter grid\n",
    "param = {\n",
    "    'max_depth': [2,3,4],\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'learning_rate': [0.2, 0.5],\n",
    "#     'subsample': [0.8, 1],\n",
    "#     'colsample_bytree': [0.8, 1]\n",
    "}\n",
    "\n",
    "# Define the scorer\n",
    "mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "\n",
    "#------------- Set up GridSearchCV -------- # took me fking 4 hrs \n",
    "# grid_search = GridSearchCV(estimator=xgb_model, param_grid=param, cv = 5, scoring=mse_scorer, verbose=3, n_jobs=-1)\n",
    "\n",
    "# # Fit the GridSearchCV to find the best parameters\n",
    "# grid_search.fit(train_X, train_y)\n",
    "\n",
    "\n",
    "# ------------- Randomized search -----------#\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distributions = {\n",
    "    'max_depth': [4, 5,6],  # 最大深度\n",
    "   'min_child_weight': [ 3, 7, 10],  # 增加最小子节点所需的数据量\n",
    "    'subsample': [0.7, 0.9],  # 行采样比例\n",
    "    'colsample_bytree': [0.5, 0.9],  # 列采样比例\n",
    "    'learning_rate': [0.01, 0.02, 0.05],  # 学习率\n",
    "    'n_estimators': [300,500,700]  # 增加树的数量\n",
    "}\n",
    "\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_distributions, n_iter=50, scoring=mse_scorer, cv=5, verbose=3, random_state=0, n_jobs=-1)\n",
    "\n",
    "\n",
    "random_search.fit(train_X, train_y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Display the best parameters and best score\n",
    "# print(\"Best parameters found:\", grid_search.best_params_)\n",
    "# print(\"Best mean squared error:\", -grid_search.best_score_)\n",
    "\n",
    "\n",
    "print(\"Best parameters found:\", random_search.best_params_)\n",
    "print(\"Best mean squared error:\", -random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "# Retrieve the best estimator (model) found by GridSearchCV\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Evaluate the model using the test set\n",
    "predictions = best_model.predict(test_X)\n",
    "mse_test = mean_squared_error(test_y, predictions)\n",
    "print(\"Mean Squared Error on the Test Set:\", mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = best_model.feature_importances_\n",
    "features = pd.Series(importance, index=train_X.columns).sort_values(ascending=True).head(20)\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "features.plot(kind='barh')\n",
    "plt.title('Top 20 Important Features')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance Score')\n",
    "plt.savefig('fm.png', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_sales_m3 = np.array(predictions) * sell_price\n",
    "promo_sales_m3 = np.array(predictions) * 0.1 * np.array(merged_df_test['is_promo']) * promo_price\n",
    "sales_amount_m3 = regular_sales_m3 + promo_sales_m3\n",
    "gross_profit_test_m3 = sales_amount_m3 - (ind_cogs * np.array(predictions))\n",
    "print(mean_squared_error(gross_profit_test_m3 ,gross_profit, squared= False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Assuming `xgb_model` is your trained XGBoost model\n",
    "# `X_test` is your test features, `y_test` is your test target\n",
    "\n",
    "# Predict using the trained model\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared = r2_score(test_y, predictions)\n",
    "\n",
    "# Number of observations\n",
    "n = test_X.shape[0]\n",
    "\n",
    "# Number of features (predictors)\n",
    "p = test_X.shape[1]\n",
    "\n",
    "# Calculate Adjusted R-squared\n",
    "adjusted_r_squared = 1 - (((1 - r_squared) * (n - 1)) / (n - p - 1))\n",
    "\n",
    "print(\"R-squared:\", r_squared)\n",
    "print(\"Adjusted R-squared:\", adjusted_r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals\n",
    "residuals = test_y - predictions\n",
    "\n",
    "# Plotting the residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(predictions, residuals, color='blue', alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot of test set')\n",
    "plt.savefig('residual.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
